[package]
name = "ifainew"
version = "0.2.7"
description = "A Tauri App"
authors = ["you"]
edition = "2021"

[features]
default = ["community"]
community = []
commercial = ["ifainew-core"]  # Enable ifainew-core integration
local-llm = ["ifainew-core"]  # Standalone local LLM support (tool parsing only)
# LLM inference using llama.cpp (GGUF native support)
llm-inference = ["llama-cpp-2", "num_cpus"]
# Combined features
commercial-local-llm = ["commercial", "llm-inference"]  # Commercial + local LLM inference


# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
# The `_lib` suffix may seem redundant but it is necessary
# to make the lib name unique and wouldn't conflict with the bin name.
# This seems to be only an issue on Windows, see https://github.com/rust-lang/cargo/issues/8519
name = "ifainew_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
tauri = { version = "2", features = [] }
tauri-plugin-opener = "2"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
tauri-plugin-fs = "2"
tauri-plugin-dialog = "2"
tauri-plugin-shell = "2"
tauri-plugin-os = "2"
reqwest = { version = "0.12.25", features = ["json", "stream", "rustls-tls"] }
eventsource-stream = "0.2.3"
futures = "0.3.31"
walkdir = "2.5.0"
portable-pty = "0.9.0"
grep = "0.4.1"
ignore = "0.4.25"
glob = "0.3.2"
termcolor = "1.4.1"
git2 = { version = "0.20.3", features = ["vendored-openssl"] }
bytes = "1.11.0"
tokio = { version = "1.48.0", features = ["full"] }
fastembed = "5.4.0"
text-splitter = "0.28.0"
anyhow = "1.0.100"
once_cell = "1.21.3"
uuid = { version = "1.19.0", features = ["v4"] }
notify = "8.2.0"
bincode = { version = "2.0.1", features = ["serde"] }
ifainew-core = { path = "../../ifainew-core/rust", optional = true }
tauri-plugin-window-state = "2"
handlebars = "6.3.2"
async-trait = "0.1.89"
serde_yaml = "0.9.34"
rust-embed = "8.9.0"
chrono = "0.4.42"
tiktoken-rs = "0.9.1"
regex = "1.12.2"
dirs = "5.0"

# llama.cpp Rust bindings for local LLM inference
# Supports GGUF format natively (user's model: qwen2.5-coder-0.5b-ifai-v3-Q4_K_M.gguf)
# Using llama-cpp-2 which has better Qwen2 support
llama-cpp-2 = { version = "0.1", optional = true }
num_cpus = { version = "1.16", optional = true }

# tracing for debugging (dev dependency only)
[dev-dependencies]
tracing-subscriber = "0.3"
