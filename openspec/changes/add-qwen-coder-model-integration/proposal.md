# 提案：集成 Qwen2.5-Coder 优化模型以实现完全离线 AI 能力

## Why（为什么）

若爱（IfAI）作为 AI 原生代码编辑器，目前存在以下核心问题：

1. **依赖外部 API 服务**：当前完全依赖 OpenAI、Claude、智谱 AI 等外部 API，无法实现真正的离线工作
2. **隐私和数据安全顾虑**：用户的代码需要发送到外部服务器，存在隐私泄露风险
3. **网络依赖性强**：无网络环境下无法使用 AI 功能，降低了可用性
4. **成本持续累积**：每次 AI 调用都会产生 API 费用，长期使用成本较高
5. **安装包体积较大**：当前 embedding 模型（all-MiniLM-L6-v2，87MB）虽然可用，但未针对代码场景优化
6. **内存占用待优化**：运行时内存占用可进一步降低，尤其在低配设备上

**机遇**：Qwen2.5-Coder 系列是阿里云通义千问团队专门针对代码场景优化的开源模型，具有以下优势：
- **轻量高效**：0.5B 参数版本体积小（~300MB 量化后），内存占用低（~512MB）
- **代码专精**：专门针对代码理解、生成、补全场景训练
- **开源免费**：支持商业使用，无 API 调用成本
- **完全离线**：可打包到应用中，实现零网络依赖
- **性能优异**：在代码任务上表现接近 GPT-3.5-Turbo

通过集成优化后的 Qwen2.5-Coder 模型，若爱可以实现：
- ✅ 完全离线的 AI 代码补全和生成
- ✅ 本地化的代码语义搜索（RAG）
- ✅ 隐私保护的代码分析
- ✅ 零成本的 AI 功能使用

## What Changes（变更内容）

### 核心功能模块

#### 1. 本地 LLM 集成系统

**集成 Qwen2.5-Coder-0.5B-Instruct（优化版）**
- 采用 4-bit 量化（GGUF 格式）将模型体积从 ~1GB 压缩至 ~300MB
- 使用 llama.cpp / candle 提供 Rust 原生推理引擎
- 支持 CPU 和 GPU（Metal/CUDA/Vulkan）硬件加速
- 实现流式响应以保持与当前 API 模式一致的用户体验
- 提供智能上下文管理（最大 8K tokens）

**功能特性**：
- 本地代码生成和补全
- 代码解释和重构建议
- 单元测试生成
- 代码审查和优化建议
- 支持混合模式：本地模型 + 云端 API（用户可选）

#### 2. 优化的代码 Embedding 系统

**升级到代码专用 Embedding 模型**
- 替换通用模型（all-MiniLM-L6-v2）为代码专用模型
- 候选方案：
  - **CodeBERT** (~140MB)：微软开源，针对代码语义理解优化
  - **GraphCodeBERT** (~150MB)：增强代码结构理解
  - **Qwen2-Embedder** (~120MB)：与 Qwen2.5-Coder 同源，语义对齐更好
- 量化优化：使用 ONNX 量化至 INT8，体积减小 ~40%
- 性能提升：代码相似度检索精度提升 15-25%

**功能增强**：
- 更精准的代码语义搜索（@codebase 查询）
- 代码片段相似度检测
- API 使用示例推荐
- 跨文件依赖分析

#### 3. 模型管理系统

**统一的本地模型管理**
- 模型下载和缓存管理
- 模型版本控制和自动更新
- 多模型切换支持（用户可选择不同大小的模型）
- 模型性能监控和优化建议

**用户界面**：
- 设置页面新增"本地 AI 模型"配置面板
- 显示模型下载进度、磁盘占用、内存使用
- 支持一键下载/删除模型
- 提供模型性能测试工具

#### 4. 混合推理架构

**智能路由系统**
- 根据任务复杂度自动选择本地模型或云端 API
- 简单任务（代码补全、格式化）→ 本地模型
- 复杂任务（架构设计、深度重构）→ 云端 API
- 用户可手动覆盖选择

**性能优化**：
- 本地模型预加载和缓存
- 推理请求队列管理
- 批处理优化（embedding 批量计算）
- 内存动态管理（根据可用内存调整模型加载）

### 技术架构变更

#### 前端 (React/TypeScript)
- 新增 `LocalModelManager` 组件：本地模型配置和监控 UI
- 扩展 `AIChat` 组件：支持显示模型来源（本地/云端）
- 新增 `ModelPerformanceMonitor` 组件：实时性能指标面板
- 扩展 Zustand store: `localModelStore`, `modelConfigStore`

#### 后端 (Rust/Tauri)
- 新增 `local_llm` 模块：本地 LLM 推理引擎集成
  - 子模块：`llama_cpp_bridge`（llama.cpp 绑定）
  - 子模块：`candle_inference`（Rust 原生推理）
  - 子模块：`model_loader`（模型加载和缓存）
- 扩展 `prompt_manager` 模块：支持本地模型的提示词优化
- 新增 `model_manager` 模块：模型下载、版本管理、缓存清理
- 扩展 RAG 模块：集成代码专用 embedding 模型
- 新增 Tauri 命令：
  - `load_local_model`, `unload_local_model`, `switch_model`
  - `local_inference`, `get_model_status`, `download_model`
  - `update_embedding_model`, `optimize_model_cache`

#### ifainew-core 包集成
- 扩展 AI 模型接口以支持：
  - 本地模型推理路径
  - 模型能力检测（支持的任务类型）
  - 混合推理路由逻辑
- 添加模型性能分析工具

### 数据模型变更

#### 新增配置文件
```
.ifai/
├── models/                   # 本地模型存储
│   ├── llm/                 # LLM 模型
│   │   └── qwen2.5-coder-0.5b-q4.gguf
│   └── embedding/           # Embedding 模型
│       └── qwen2-embedder-onnx/
├── model-config.json        # 模型配置
└── model-cache/             # 推理缓存
```

#### 新增数据结构
- `LocalModelConfig`: 本地模型配置（路径、量化参数、推理设置）
- `ModelPerformanceMetrics`: 性能指标（延迟、吞吐、内存占用）
- `InferenceRequest`: 推理请求（任务类型、优先级、超时设置）
- `ModelRouter`: 路由策略（本地/云端决策逻辑）

### UI/UX 变更

#### 新增界面功能
1. **本地 AI 模型设置**
   - 设置页面新增"本地 AI 模型"标签页
   - 模型列表和状态显示
   - 下载进度和磁盘空间监控
   - 推理性能配置（线程数、GPU 加速）

2. **AI 对话增强**
   - 消息中显示模型来源标识（🏠 本地 / ☁️ 云端）
   - 显示推理性能指标（延迟、tokens/s）
   - 支持手动切换模型

3. **性能监控面板**
   - 实时显示 CPU/GPU/内存使用
   - 推理任务队列状态
   - 模型缓存命中率

### Breaking Changes（破坏性变更）

**BREAKING**: AI 模型配置格式变更
- 旧版本：仅支持外部 API 配置
- 新版本：支持本地模型 + 外部 API 混合配置
- 迁移方案：自动检测并升级配置文件，保持向后兼容

**BREAKING**: 最低系统要求提升
- 新增要求：至少 4GB RAM（推荐 8GB）
- 新增要求：至少 2GB 可用磁盘空间（用于模型存储）
- macOS: 需要 Metal 支持（macOS 11+）
- Windows: 推荐支持 DirectML 的 GPU
- Linux: 推荐支持 Vulkan 的 GPU

## Impact（影响范围）

### 受影响的规格说明（Affected Specs）
- **ai-chat**: AI 对话功能（新增本地模型支持、混合推理）
- **rag-embedding**: RAG 嵌入系统（升级到代码专用模型）
- **新增**: `ai-model-integration` - AI 模型集成
- **新增**: `local-llm-inference` - 本地 LLM 推理
- **新增**: `model-management` - 模型管理

### 受影响的代码（Affected Code）
- **前端核心文件** (8+ 文件):
  - `src/components/AIChat/index.tsx` - 对话组件
  - `src/stores/chatStore.ts` - 对话状态管理
  - 新增模型管理和监控组件
- **后端核心文件** (12+ 文件):
  - `src-tauri/src/lib.rs` - 新命令注册
  - `src-tauri/src/ai_utils.rs` - AI 工具集成
  - 新增本地 LLM 和模型管理模块
- **ifainew-core 包**:
  - 需要扩展本地模型接口
- **依赖变更**:
  - 新增 `llama-cpp-rs` 或 `candle` 依赖
  - 升级 `fastembed` 或替换为自定义 embedding 引擎

### 性能影响

**安装包体积变化**：
- 当前：~50MB（不含模型）
- 新版：~350MB（含量化 LLM + Embedding 模型）
- 增量：~300MB

**运行时内存变化**：
- 当前：~80MB（不含 AI 模型）
- 新版（仅 Embedding）：~200MB（模型加载时）
- 新版（LLM + Embedding）：~700MB（全量加载）
- 优化策略：按需加载，空闲时卸载

**推理性能**（Qwen2.5-Coder-0.5B-Q4）：
- CPU 推理：~10-15 tokens/s（8 核 CPU）
- GPU 推理（Metal/CUDA）：~30-50 tokens/s
- Embedding 推理：<100ms/文档

### 用户体验提升
1. **隐私保护**：代码完全本地处理，无需担心泄露
2. **离线可用**：无网络环境下也能使用 AI 功能
3. **零成本使用**：无 API 调用费用
4. **响应速度**：本地推理延迟更低（无网络往返）
5. **自主可控**：用户可选择模型大小和性能级别

### 风险和缓解措施
1. **体积风险**：安装包增大可能影响下载体验
   - 缓解：提供"轻量版"（仅 embedding）和"完整版"（含 LLM）两个安装包
2. **性能风险**：低配设备上可能推理缓慢
   - 缓解：自动检测硬件，低配设备默认使用云端 API，提供性能模式切换
3. **兼容性风险**：不同平台推理性能差异大
   - 缓解：提供平台优化版本（Metal/CUDA/Vulkan），CPU 回退方案
4. **质量风险**：本地小模型质量可能不如大型 API 模型
   - 缓解：智能任务路由，复杂任务自动使用云端 API，用户可手动覆盖

## Success Metrics（成功指标）

1. **功能完整性**:
   - ✓ 本地 LLM 推理成功率 > 95%
   - ✓ Embedding 模型切换无缝
   - ✓ 混合推理路由准确率 > 90%
   - ✓ 模型下载和加载成功率 > 99%

2. **性能指标**:
   - ✓ 本地代码补全延迟 < 200ms
   - ✓ Embedding 检索延迟 < 150ms
   - ✓ LLM 流式响应首 token 延迟 < 1s
   - ✓ 内存占用（空闲）< 300MB

3. **用户体验**:
   - ✓ 离线模式可用性 > 95%
   - ✓ 用户满意度调查 > 4.2/5
   - ✓ 本地模型使用率 > 60%（在可用情况下）
   - ✓ 模型切换响应时间 < 3s

4. **稳定性**:
   - ✓ 本地推理崩溃率 < 0.5%
   - ✓ 模型加载失败率 < 2%
   - ✓ 混合模式无缝切换成功率 > 98%

## Alignment with Project Vision（与项目愿景的对齐）

若爱（IfAI）的核心愿景是"让 AI 成为开发者最贴心的编程伴侣"。此提案与愿景高度对齐：

1. **"本地优先"**：完全实现离线 AI 能力，体现隐私保护理念
2. **"极致性能"**：Rust 原生推理引擎，保持高性能和低内存占用
3. **"开放生态"**：使用开源模型，支持社区贡献和定制
4. **"隐私保护"**：代码不离开本地，符合企业级安全要求

同时，此提案符合项目的技术哲学：
- **轻量高效**: 量化模型和优化推理保持应用轻量
- **本地优先**: 真正实现本地 AI 处理
- **开放可扩展**: 支持多种模型和推理引擎

## Next Steps（后续步骤）

1. **评审和反馈** (3 天)
   - 团队评审此提案
   - 收集用户和社区反馈
   - 调整技术方案细节

2. **技术预研** (1 周)
   - 验证 llama.cpp 和 candle 在 Tauri 中的集成可行性
   - 测试不同量化版本的性能和质量
   - 评估不同平台的推理性能
   - 选择最优的 embedding 模型

3. **启动开发** (4-6 周)
   - 按照 tasks.md 中的任务清单逐步实施
   - 每周发布内部测试版本
   - 持续收集性能数据并优化

4. **Beta 测试** (2 周)
   - 发布公开 beta 版本（提供轻量版和完整版）
   - 收集用户反馈和性能数据
   - 针对不同硬件平台优化

5. **正式发布** (v0.3.0)
   - 完整文档和使用指南
   - 模型下载和管理工具
   - 社区支持和模型生态建设
